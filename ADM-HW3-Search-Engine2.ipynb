{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "import json\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):#input: a text to be clean:\n",
    "                    #output: a list of words of the text cleaned\n",
    "    text=text.lower()   \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    clean_text=\"\".join([ch if ch.isalnum() else \" \" for ch in text]) #trasform all non alnumeric character\n",
    "                                                                    #into a space\n",
    "    words = word_tokenize(clean_text) \n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # stemming of words\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(w) for w in words]\n",
    "    \n",
    "    return(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={}\n",
    "index2={}\n",
    "al=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vocab='/home/tiago/Scrivania/Libri Magistrale/1st semester/ADM/HomeWork3/vocabulary.pkl'\n",
    "path_index='/home/tiago/Scrivania/Libri Magistrale/1st semester/ADM/HomeWork3/inverted_index2.pkl'\n",
    "path_tsv='/home/tiago/Scrivania/Libri Magistrale/1st semester/ADM/HomeWork3/fileTsv/'\n",
    "path_html='/home/tiago/Scrivania/Libri Magistrale/1st semester/ADM/HomeWork3/htmls.txt'\n",
    "\n",
    "\n",
    "with open(path_index, 'rb') as f:\n",
    "    index2=pickle.load(f)\n",
    "with open(path_vocab, 'rb') as f:\n",
    "    vocab=pickle.load(f)\n",
    "with open(path_html, 'r') as f:\n",
    "    al=json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_tfidf(query_words, term):\n",
    "    freq_in_doc=1/len(query_words)\n",
    "    num_of_words_in_doc=len(query_words)\n",
    "    num_docs=30000\n",
    "    num_docs_with_w=len(index2[vocab[term]])\n",
    "    tf=freq_in_doc/num_of_words_in_doc\n",
    "    idf=math.log(num_docs/num_docs_with_w)\n",
    "\n",
    "    return tf*idf\n",
    "\n",
    "def get_d_tfidf(term, doc):\n",
    "    return index2[vocab[term]][doc]\n",
    "\n",
    "\n",
    "def cosine(query_words, doc):\n",
    "    #numerator\n",
    "    numerator=0\n",
    "    sum_q=0\n",
    "    sum_d=0\n",
    "    for term in query_words:\n",
    "        q_tfidf=get_q_tfidf(query_words, term)\n",
    "        d_tfidf=get_d_tfidf(term, doc)\n",
    "        \n",
    "        numerator+=(q_tfidf*d_tfidf)\n",
    "        sum_q=sum_q+(q_tfidf**2)\n",
    "        sum_d=sum_d+(d_tfidf**2)\n",
    "    sum_q_sqrt=math.sqrt(sum_q)\n",
    "    sum_d_sqrt=math.sqrt(sum_d)\n",
    "    \n",
    "    denominator=sum_q_sqrt*sum_d_sqrt\n",
    "    print('num:', numerator, 'den: ', denominator, 'sum_q: ', sum_q_sqrt, 'sum_d: ', sum_d_sqrt)\n",
    "    return numerator/denominator   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Give me a query :) more love\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num: 0.20210713350422746 den:  0.20210713350422746 sum_q:  7.7440033031827555 sum_d:  0.026098534\n",
      "num: 0.039542700706827395 den:  0.039542700706827395 sum_q:  7.7440033031827555 sum_d:  0.005106235\n",
      "num: 0.07308338067750979 den:  0.07308338067750979 sum_q:  7.7440033031827555 sum_d:  0.009437416\n",
      "num: 0.03907082760955126 den:  0.03907082760955126 sum_q:  7.7440033031827555 sum_d:  0.005045301\n",
      "num: 0.14112653034112327 den:  0.14112653034112327 sum_q:  7.7440033031827555 sum_d:  0.018223976\n",
      "num: 0.0750948545594983 den:  0.0750948545594983 sum_q:  7.7440033031827555 sum_d:  0.009697162\n",
      "num: 0.024804058068100973 den:  0.024804058068100973 sum_q:  7.7440033031827555 sum_d:  0.003203002\n",
      "num: 0.06496300332578574 den:  0.06496300332578574 sum_q:  7.7440033031827555 sum_d:  0.008388814\n",
      "num: 0.03054231030773627 den:  0.03054231030773627 sum_q:  7.7440033031827555 sum_d:  0.003943995\n",
      "num: 0.07820387054164028 den:  0.07820387054164028 sum_q:  7.7440033031827555 sum_d:  0.010098636\n",
      "num: 0.11862810212048219 den:  0.11862810212048219 sum_q:  7.7440033031827555 sum_d:  0.015318705\n",
      "num: 0.014210656493515425 den:  0.014210656493515425 sum_q:  7.7440033031827555 sum_d:  0.001835053\n",
      "num: 0.007565008410832989 den:  0.007565008410832989 sum_q:  7.7440033031827555 sum_d:  0.000976886\n",
      "We have found 13 results in 0.02 second(s).\n",
      "                            title  \\\n",
      "0   Love by the Light of the Moon   \n",
      "1                           Class   \n",
      "2            Consolation Marriage   \n",
      "3                   The Dead Zone   \n",
      "4                   Second Choice   \n",
      "5                       All Woman   \n",
      "6                      Easy Money   \n",
      "7           I Found Stella Parish   \n",
      "8             All the Right Moves   \n",
      "9                   Baby It's You   \n",
      "10                       Hercules   \n",
      "11                  The Big Chill   \n",
      "12                  Chicken Ranch   \n",
      "\n",
      "                                                intro  \\\n",
      "0   Love by the Light of the Moon is a 1901 film b...   \n",
      "1   Class is a 1983 American comedy film directed ...   \n",
      "2   Consolation Marriage is a 1931 American Pre-Co...   \n",
      "3   The Dead Zone is a 1983 American horror thrill...   \n",
      "4   Second Choice is a 1930 American Pre-Code blac...   \n",
      "5   All Woman  is a 1918 American comedy film dire...   \n",
      "6   Easy Money is a 1983 American comedy film star...   \n",
      "7   I Found Stella Parish is a 1935 melodrama star...   \n",
      "8   All the Right Moves is a 1983 American sports ...   \n",
      "9   Baby It You is a 1983 American romantic comedy...   \n",
      "10  Hercules is a 1983  science fiction-fantasy ad...   \n",
      "11  The Big Chill is a 1983 American comedy-drama ...   \n",
      "12  Chicken Ranch is a 1983 documentary film by Ni...   \n",
      "\n",
      "                                                  url  similarity  \n",
      "0   https://en.wikipedia.org/wiki/Love_by_the_Ligh...         1.0  \n",
      "1          https://en.wikipedia.org/wiki/Class_(film)         1.0  \n",
      "2   https://en.wikipedia.org/wiki/The_Squaw_Man_(1...         1.0  \n",
      "3   https://en.wikipedia.org/wiki/The_Dead_Zone_(f...         1.0  \n",
      "4   https://en.wikipedia.org/wiki/From_Leadville_t...         1.0  \n",
      "5   https://en.wikipedia.org/wiki/The_Sap_from_Syr...         1.0  \n",
      "6   https://en.wikipedia.org/wiki/Easy_Money_(1983...         1.0  \n",
      "7         https://en.wikipedia.org/wiki/Sarah_and_Son         1.0  \n",
      "8   https://en.wikipedia.org/wiki/All_the_Right_Mo...         1.0  \n",
      "9   https://en.wikipedia.org/wiki/Baby_It%27s_You_...         1.0  \n",
      "10  https://en.wikipedia.org/wiki/Hercules_(1983_f...         1.0  \n",
      "11  https://en.wikipedia.org/wiki/The_Big_Chill_(f...         1.0  \n",
      "12  https://en.wikipedia.org/wiki/Chicken_Ranch_(f...         1.0  \n"
     ]
    }
   ],
   "source": [
    "query=input('Give me a query :)')\n",
    "\n",
    "query_words=set(cleaner(query))\n",
    "posts_list=[]\n",
    "start=time.time()\n",
    "for word in query_words: # iterate over words\n",
    "    if word in vocab.keys():#check if the word is present\n",
    "        num_word=vocab[word]#retrive the number of the word\n",
    "        posts_list.append(index[num_word])#retrive the doducemnts of the word\n",
    "\n",
    "conjuntive_docs=set()\n",
    "posts_list.sort(key=len) #sorting by the len of the dictionary inside\n",
    "if len(posts_list)>0:\n",
    "    conjuntive_docs=set(posts_list[0]) #take the smallest\n",
    "    for posts in posts_list[1:]: #iterate over the documents from 1 to the end\n",
    "        conjuntive_docs.intersection_update(set(posts.keys())) #update the set with commons documents\n",
    "\n",
    "docs=list(conjuntive_docs)\n",
    "output=[]\n",
    "for doc in docs:#iterate over documents\n",
    "    name_file_tsv=''.join(['article_', str(doc), '.tsv']) #obtain the name of the file\n",
    "    file_tsv=None\n",
    "    out_doc=[]\n",
    "    with open(path_tsv+name_file_tsv, 'r') as file: #open the file\n",
    "        file_tsv_reader=csv.reader(file, delimiter='\\t') #read it\n",
    "        list_file_content=next(file_tsv_reader)#read the only one line\n",
    "        #print(type(list_file_content))\n",
    "        \n",
    "        out_doc.append(list_file_content[0]) #append title\n",
    "        out_doc.append(list_file_content[1])#append intro\n",
    "        out_doc.append(al[doc])\n",
    "        out_doc.append(cosine(query_words, doc))\n",
    "        \n",
    "\n",
    "    output.append(out_doc)\n",
    "seconds=round(time.time()-start, 2)\n",
    "if len(output)>0:\n",
    "    print('We have found {} results in {} second(s).'.format(len(output), seconds))\n",
    "    output_df=pd.DataFrame(output, columns=['title', 'intro', 'url', 'similarity'])\n",
    "    print(output_df)\n",
    "else:\n",
    "    print('No results.')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
