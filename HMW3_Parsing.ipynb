{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide to code:\n",
    "\n",
    "We must underline that to find each element we take advantage of the Wikipedia website structure: we have seen that this structure is more or less always the same. We add if conditions to overcome the changes in the structure that didn't allow us to find the data. For what we have just explained, we warmly recomend to use wikipedia API to get intro and plot and for the info_box it would be better to find a solution that would be able to find data in any circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import collections\n",
    "import csv\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to get the info box:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide to code:\n",
    "\n",
    "First two ifs statements need to avoid two kind of pages that we are not interested in (in order): pages that don't exist anymore (we return -1, because later it will take less time to be found than a string) and pages that should bring the user to other websites (we put \"NA\", because in this case we want to add at least the title).\n",
    "\n",
    "We have seen that the intro is always placed after the infobox table, so we start to import the paragraphs of the intro from there. However, if there isn't the infobox the intro starts after the line that defines the main section of the page, in this case we should use nextChild function instead nextSibling because of the hirarchy of tags. We will loop till we find p tags (the tags for paragraphs), as soon as the tag changes we will stop the cycle and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intro(site):\n",
    "\n",
    "    # Deal with Page not FOund:\n",
    "    \n",
    "    if \"Page not Found\" is in site.text: # If the isn't the page we return -1 it takes less time to find it later\n",
    "        return -1\n",
    "    \n",
    "    # Deal with Disanbiguation page:\n",
    "    \n",
    "    disanbiguation = site.find(\"a\", {\"title\": \"Category:Disambiguation pages\"})\n",
    "    if disanbiguation: # Disanbiguation page = a page that sends you to other pages (we decide to skip them, we add only the title)\n",
    "        return \"NA\"\n",
    "    \n",
    "    # Leet's start to find intro:\n",
    "    \n",
    "    intro_lst = [] # A list which will contain each paragraph\n",
    "    par_before_intro = site.find(\"table\", {\"class\": \"infobox vevent\"}) #Â It is the element before the paragrphs we need\n",
    "    \n",
    "    if par_before_intro:\n",
    "        el = par_before_intro.find_next_sibling()  # The element changes everytime the cycle ends (at the beginning we start from the line after table tag\n",
    "    else:  # If there isn't the info box, we can find the intro right after the following tag (but it is a child of this)\n",
    "        el = site.find(\"div\", {\"class\": \"mw-parser-output\"}).findChild()\n",
    "        \n",
    "    while True:\n",
    "        \n",
    "        try:\n",
    "            el_name = el.name\n",
    "        except:\n",
    "            el_name = \"\" # In case we don't get the tag \n",
    "\n",
    "        if el_name != \"p\": # We stop when we found the first tag different from p\n",
    "            intro = \"\\n\".join(intro_lst) # So we create a string with the whole text\n",
    "            break # And stop the cycle\n",
    "        else:\n",
    "            intro_lst.append(el.text) # We append that part as text in our list\n",
    "            el = el.find_next_sibling() # And we switch to the next element\n",
    "        \n",
    "    return intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to get the plot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide to code:\n",
    "\n",
    "As for the intro, the first two lines are made to deal with pages that we don't need.\n",
    "\n",
    "We have noticed that the paragphs of the plot start after the the title plot (contained in h2, whith the name write in span tag in h2), this title cna slightly changes, so we have written different if stataments for each possible event. \n",
    "\n",
    "Notice that the list plot_par contains always only element, which is the one descibed above (for construction).\n",
    "Furthermore, it can happen that thereisn't the plot, in that case we just return a NA.\n",
    "\n",
    "As for intro, after that so we start to import the paragraphs from the next sibling of what we wrote above.  We will loop till we find p tags (the tags for paragraphs), as soon as the tag changes we will stop the cycle and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_plot(site):\n",
    "\n",
    "    # Deal with page not found\n",
    "    \n",
    "    if \"Page not Found\" is in site.text: # If the isn't the page we return -1 it takes less time to find it later\n",
    "        return -1\n",
    "    \n",
    "    # Deal with disanbiguation page:\n",
    "    \n",
    "    disanbiguation = site.find(\"a\", {\"title\": \"Category:Disambiguation pages\"})\n",
    "    if disanbiguation: # Disanbiguation page = a page that sends the user to other pages (we decide to skip them, we add only the title)\n",
    "        return \"NA\"\n",
    "    \n",
    "    # Let's start to find the plot\n",
    "    \n",
    "    plot_lst = [] # A list which will contain each paragraph\n",
    "    paragraphs = site.find_all(\"h2\") # All elements in site that have the tag \"h2\"\n",
    "    plot_par = [i for i in paragraphs if (i.find(\"span\", {\"id\": \"Plot\"}) or i.find(\"span\", {\"id\": \"Plot_summary\"})\n",
    "                                          or i.find(\"span\", {\"id\": \"Premise\"}) or i.find(\"span\", {\"id\": \"Premise\"}))]\n",
    "    \n",
    "    if not plot_par: # If there isn't the plot paragraph\n",
    "        return \"NA\"\n",
    "    \n",
    "    el = plot_par[0].find_next_sibling() # The element changes everytime the cycle ends (at the beginning we start from the line after h2 tag)\n",
    "    while True:\n",
    "\n",
    "        try:\n",
    "            el_name = el.name\n",
    "        except:\n",
    "            el_name = \"\" # In case we don't get the tag \n",
    "\n",
    "        if el_name != \"p\": # When occurs the first tag different from p we stop the loop\n",
    "            plot = \"\\n\".join(plot_lst) # So we create a string with the whole text\n",
    "            break # And stop the cycle\n",
    "        else:\n",
    "            plot_lst.append(el.text) # We append that part as text in our list\n",
    "            el = el.find_next_sibling() # And we switch to the next element\n",
    "\n",
    "    return plot    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let' create a function to get info box's data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide to code:\n",
    "\n",
    "Whit this function we want get from the infobox only the informatio asked from the exercise, so we have created a dictionary which contains as key what the information is about and as values \"NA\", which will be replace with the data that we will find in the infobox.\n",
    "\n",
    "Also in this case we deal with page not found and disanbiguation page, but this time we will return a dictionary and simply a number/string (instead the final function to produce file.tsv won't work).\n",
    "\n",
    "After getting the infobox table, we get the object title from the tag \"tr\" and the value from tag \"th\". We have developed some if statements to add each element to correct key in the dictionary. For the Title, it seems that this is the only element that is a \"values\" = \"NA\" (but we cannot 100% that it work for every files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_info(site):\n",
    "\n",
    "# Let's create a dictionary whith the neeed information\n",
    "    \n",
    "    info_dict = {\"Title\": \"NA\",\n",
    "                \"Directed by\": \"NA\",\n",
    "                \"Produced by\": \"NA\",\n",
    "                \"Written by\": \"NA\",\n",
    "                \"Starring\": \"NA\",\n",
    "                \"Music by\": \"NA\",\n",
    "                \"Release date\": \"NA\",\n",
    "                \"Country\": \"NA\",\n",
    "                \"Language\": \"NA\",\n",
    "                \"Budget\": \"NA\" \n",
    "                }\n",
    "    \n",
    "    # Deal with Page not found:\n",
    "    \n",
    "    if \"Page not Found\" is in site.text: # If the isn't the page we return -1 it takes less time to find it later\n",
    "        for key, _ in info_dict.items():\n",
    "            info_dict[key] = -1\n",
    "        return info_dict\n",
    "    \n",
    "    # Deal with disanbiguation page:\n",
    "    \n",
    "    disanbiguation = site.find(\"a\", {\"title\": \"Category:Disambiguation pages\"})\n",
    "    if disanbiguation: # Disanbiguation page = a page that sends you to other pages (we decide to skip them, we add only the title)\n",
    "        info_dict[\"Title\"] = site.find(\"h1\").text\n",
    "        return info_dict\n",
    "    \n",
    "    # Let's loop:\n",
    "    \n",
    "    table = site.find('table', {\"class\" : \"infobox vevent\"}) # Let's get the table where the infobox is palced\n",
    "    \n",
    "    if not table: # It can happen that there isn't the infobox\n",
    "        return info_dict\n",
    "    \n",
    "    rows = table.find_all(\"tr\") # It represents the table records\n",
    "                \n",
    "    for el in rows:\n",
    "\n",
    "        try:\n",
    "            key = el.find(\"th\").get_text(strip=True) # They are the Titles (Ex. \"Prodced by\")\n",
    "        except:\n",
    "            key = \"NA\" \n",
    "\n",
    "        try:\n",
    "            values = el.find(\"td\").get_text(strip=True) # The attributes of each title\n",
    "        except:\n",
    "            values = \"NA\"\n",
    "\n",
    "        if values == \"NA\": # It seems title is the only one that has values == \"NA\"\n",
    "            \n",
    "            info_dict[\"Title\"] = key\n",
    "                    \n",
    "        elif key == \"Directed by\":\n",
    "            \n",
    "            info_dict[key] = values\n",
    "            \n",
    "        elif key == \"Produced by\":\n",
    "            \n",
    "            info_dict[key] = values\n",
    "                \n",
    "        elif key == \"Written by\":\n",
    "            \n",
    "            info_dict[key] = values\n",
    "        \n",
    "        elif key == \"Starring\":\n",
    "            \n",
    "            info_dict[key] = values\n",
    "       \n",
    "        elif key == \"Music by\":\n",
    "            \n",
    "            info_dict[key] = values\n",
    "        \n",
    "        elif key == \"Release date\":\n",
    "            \n",
    "            info_dict[key] = values\n",
    "        \n",
    "        elif key == \"Running time\":\n",
    "         \n",
    "            info_dict[key] = values\n",
    "        \n",
    "        elif key == \"Country\":\n",
    "\n",
    "            info_dict[key] = values\n",
    "        \n",
    "        elif key == \"Language\":\n",
    "\n",
    "            info_dict[key] = values\n",
    "        \n",
    "        elif key == \"Budget\":\n",
    "\n",
    "            info_dict[key] = values\n",
    "                \n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to clean data from html symbols and from some errors that may lead to errors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide to code:\n",
    "\n",
    "In this part we are going to solve all the errors that we could observe opening some files. To be cleare this is not needed for the inverted index, but it will help to have more precise result later.\n",
    "\n",
    "For i.e. we have noticed that Wikipedia uses lables such as (\"[n]\") to refer to some external link, or sometimes after converting in text some html elements remains such as (\"\\n\",\"\\s\", ...).\n",
    "\n",
    "In addition sometimes dates are written in weired way, our aim is to get only the year, another example can be that many times name are all together (e.x. \"MarioRossiLucaFalco\" or \"Mario RossiLuca Falco\").\n",
    "\n",
    "So we are going to deal with all these kind of errors that we have seen. Of course, these are not all the errors can be occur, so we cannot guarantee that all words will be clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all(site):\n",
    "    \n",
    "    raw_intro = find_intro(site)\n",
    "    raw_plot = find_plot(site)\n",
    "    raw_info = find_info(site)\n",
    "    \n",
    "    # Let's clean the text from any html symhols and to wikipedia lables (\"[n]\") for intro an dplot:\n",
    "    \n",
    "    if type(raw_intro) is int: # It is page noy found, so we can skip it\n",
    "        intro = raw_intro\n",
    "        plot = raw_plot\n",
    "        info_dict = raw_info\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        intro_not_clean = re.sub('(?<=\\w|\\.|\\s)((\\\\\\'s)|(\\\\n){,10}|(\\\\\\'))', \"\", raw_intro) # We Remove all html symbols (\"\\'s, \\n\")\n",
    "        intro = re.sub(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', \"\", intro_not_clean) # Remove any notes (in Wikipedia it links to another page)\n",
    "\n",
    "        plot = re.sub('(?<=\\w|\\.|\\s)((\\\\\\'s)|(\\\\n){,10}|(\\\\\\'))', \"\", raw_plot) # We all Remove html symbols (\"\\'s, \\n\")\n",
    "\n",
    "        # Let's clean the infobox:\n",
    "\n",
    "        info_dict = {}\n",
    "\n",
    "        for key, value in raw_info.items():\n",
    "\n",
    "            if key == \"Title\":          \n",
    "                \n",
    "                if value != \"NA\": \n",
    "                    if re.search(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', value): # Delet any reference (ex. [1]), We'll do for each variable and put always as first if\n",
    "                        clean = re.sub(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', \"\", value)\n",
    "                        info_dict[key] = clean\n",
    "                    else:\n",
    "                        info_dict[key] = value\n",
    "                else:\n",
    "                    info_dict[key] = value                \n",
    "\n",
    "            elif key == \"Directed by\":\n",
    "\n",
    "                if value != \"NA\": \n",
    "                    if re.search(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', value): # Delet any reference (ex. [1])\n",
    "                        clean = re.sub(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', \"\", value)\n",
    "                        info_dict[key] = clean\n",
    "                    else:\n",
    "                        info_dict[key] = value    \n",
    "                else:\n",
    "                    info_dict[key] = value\n",
    "\n",
    "            elif key == \"Produced by\":\n",
    "                \n",
    "                if value != \"NA\":                    \n",
    "                    if re.search(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', value): # Delet any reference (ex. [1])\n",
    "                        clean = re.sub(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', \"\", value)   \n",
    "                        \n",
    "                        if re.search(r'(?<!\\s|\\-|\\_|\\/)(?=[A-Z])', clean):\n",
    "                            split = re.split(r'(?<!\\s|\\-|\\_|\\/)(?=[A-Z])', clean) # Divide joint name (ex. Radish CamuJohn Snow)\n",
    "                            info_dict[key] = \", \".join([i for i in split if i != \"\"]) # transform the split list a string (don't add empty stings)\n",
    "                        else:\n",
    "                            info_dict[key] = clean\n",
    "                    else:\n",
    "\n",
    "                        info_dict[key] = value                \n",
    "                else:\n",
    "                    \n",
    "                    info_dict[key] = value\n",
    "                    \n",
    "            elif key == \"Written by\":\n",
    "\n",
    "                if value != \"NA\":\n",
    "                    if re.search(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', value): # Delet any reference (ex. [1])\n",
    "                        clean = re.sub(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', \"\", value)\n",
    "                        info_dict[key] = clean\n",
    "                        \n",
    "                        if re.search(r'(?<=\\w)\\((.*?)\\)', clean):\n",
    "                            split = re.sub('(?<=\\w)\\((.*?)\\)', \"\", clean) # Remove the parentesis with the role of each person\n",
    "                            info_dict[key] = split\n",
    "                        elif re.search(r'(?<!\\s|\\-|\\_|\\/)(?=[A-Z])', clean):\n",
    "                            split = re.split(r'(?<!\\s|\\-|\\_|\\/)(?=[A-Z])', clean) # Divide joint name (ex. Radish CamuJohn Snow)\n",
    "                            info_dict[key] = \", \".join([i for i in split if i != \"\"]) # transform the split list a string (don't add empty stings)\n",
    "                        else: \n",
    "                            info_dict[key] = value\n",
    "                    else: \n",
    "                        info_dict[key] = value\n",
    "                else: \n",
    "                    info_dict[key] = value\n",
    "                            \n",
    "\n",
    "            elif key == \"Starring\":\n",
    "\n",
    "                if value != \"NA\":                    \n",
    "                    if re.search(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', value): # Delet any reference (ex. [1])\n",
    "                        clean = re.sub(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', \"\", value)\n",
    "                        info_dict[key] = clean\n",
    "                        \n",
    "                        if re.search(r'(?<!\\s|\\-|\\_|\\/)(?=[A-Z])', clean):\n",
    "                            split = re.split(r'(?<!\\s|\\-|\\_|\\/)(?=[A-Z])', clean) # Divide joint name (ex. Radish CamuJohn Snow)\n",
    "                            info_dict[key] = \", \".join([i for i in split if i != \"\"]) # transform the split list a string (don't add empty stings)\n",
    "                        else:\n",
    "                            info_dict[key] = value\n",
    "                    else:\n",
    "                        info_dict[key] = value\n",
    "                else:\n",
    "                    info_dict[key] = value\n",
    "\n",
    "            elif key == \"Music by\":\n",
    "\n",
    "                if value != \"NA\": \n",
    "                    if re.search(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', value): # Delet any reference (ex. [1]), We'll do for each variable and put always as first if\n",
    "                        clean = re.sub(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', \"\", value)\n",
    "                        info_dict[key] = clean\n",
    "                    else:\n",
    "                        info_dict[key] = value\n",
    "                else:\n",
    "                    info_dict[key] = value\n",
    "\n",
    "            elif key == \"Release date\":\n",
    "\n",
    "                if value != \"NA\":                    \n",
    "                    if re.search(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', value): # Delet any reference (ex. [1])\n",
    "                        clean = re.sub(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', \"\", value)\n",
    "                        info_dict[key] = clean\n",
    "\n",
    "                        if re.search(r'(\\(\\d+\\))', clean): \n",
    "                            year = re.sub('(\\(\\d+\\))', \"\", clean) # Get only the year (ex. 2005(2005))\n",
    "                            info_dict[key] = year\n",
    "                        elif re.search(r'(?<=\\()(\\d{4})', clean):\n",
    "                            year = re.search('(?<=\\()(\\d{4})', clean) # Get only the year when (random characters(yyyy-mm-dd))\n",
    "                            info_dict[key] = year.group()\n",
    "                        else:\n",
    "                            info_dict[key] = value\n",
    "                    else:\n",
    "                        info_dict[key] = value\n",
    "                else:\n",
    "                    info_dict[key] = value               \n",
    "\n",
    "            elif key == \"Running time\":\n",
    "\n",
    "                if value != \"NA\": \n",
    "                    if re.search(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', value): # Delet any reference (ex. [1]), We'll do for each variable and put always as first if\n",
    "                        clean = re.sub(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', \"\", value)\n",
    "                        info_dict[key] = clean\n",
    "                    else:\n",
    "                        info_dict[key] = value\n",
    "                else:\n",
    "                    info_dict[key] = value\n",
    "\n",
    "            elif key == \"Country\":\n",
    "\n",
    "                if value != \"NA\": \n",
    "                    if re.search(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', value): # Delet any reference (ex. [1]), We'll do for each variable and put always as first if\n",
    "                        clean = re.sub(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', \"\", value)\n",
    "                        info_dict[key] = clean                       \n",
    "                        \n",
    "                        if re.search(r'(?<!\\s|\\-|\\_|\\/)(?=[A-Z])', clean):\n",
    "                            split = re.split(r'(?<!\\s|\\-|\\_|\\/)(?=[A-Z])', clean) # Divide joint name (ex. Radish CamuJohn Snow)\n",
    "                            info_dict[key] = \", \".join([i for i in split if i != \"\"]) # transform the split list a string (don't add empty stings)\n",
    "                        else:\n",
    "                            info_dict[key] = clean\n",
    "                    else:\n",
    "                        info_dict[key] = value\n",
    "                else:\n",
    "                    info_dict[key] = value\n",
    "\n",
    "            elif key == \"Language\":\n",
    "\n",
    "                if value != \"NA\": \n",
    "                    if re.search(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', value): # Delet any reference (ex. [1]), We'll do for each variable and put always as first if\n",
    "                        clean = re.sub(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', \"\", value)\n",
    "                        info_dict[key] = clean\n",
    "                        \n",
    "                        if re.search(r'(?<!\\s|\\-|\\_|\\/)(?=[A-Z])', clean):\n",
    "                            split = re.split(r'(?<!\\s|\\-|\\_|\\/)(?=[A-Z])', clean) # Divide joint name (ex. Radish CamuJohn Snow)\n",
    "                            info_dict[key] = \", \".join([i for i in split if i != \"\"]) # transform the split list a string (don't add empty stings)\n",
    "                        else:\n",
    "                            info_dict[key] = clean       \n",
    "                    else:\n",
    "                        info_dict[key] = value\n",
    "                else:\n",
    "                    info_dict[key] = value\n",
    "\n",
    "            elif key == \"Budget\":\n",
    "\n",
    "                if value != \"NA\": \n",
    "                    if re.search(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', value): # Delet any reference (ex. [1]), We'll do for each variable and put always as first if\n",
    "                        clean = re.sub(r'(?<=\\w|\\.|\\s)(\\[\\d+\\]){,10}', \"\", value)\n",
    "                        info_dict[key] = clean\n",
    "                    else:\n",
    "                        info_dict[key] = value\n",
    "                else:\n",
    "                    info_dict[key] = value\n",
    "        \n",
    "    return [intro, plot, info_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find all the required element for each movie:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide to code:\n",
    "\n",
    "The first three rows are the path for where the .html file have been saved, where we want to save the .tsv file and where we want to save the log file.\n",
    "\n",
    "\"i\" represents the number of the file (it must be set depending on where we want to start), it must important to underline that it will be used to give a name to the saved file (and not to load data).\n",
    "\n",
    "In the loop we load each .html file, open with beautifulsoup and apply the function developed above to get the information required to compelte the task and write them to different .tsv files (one for each .html file). We have got the title from the infobox because we have seen that it cleaner than the one get from teh tag \"h1\". We want the title in first position, then we get it from the dictionary and put as first element in the list that will be write on the file (\"elements = [info_box[0]] + intro_plot + info_box[1:]\"). \n",
    "\n",
    "As we said our code cannot generally work, some errors may be occur, that's why we created log file in which we print each element that is loaded by the function, in this way we know that if the program stops before the end, the last element is the that gives us problem. So print it and the error given, in this way it is easier to do the debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_tsv():\n",
    "\n",
    "    path = \"/Users/Dario/Desktop/HMW3_Data/\" # Where the .html file have been saved\n",
    "    save_in = \"/Users/Dario/Desktop/HMW3_tsv/\" # Where you want to save the .tsv file\n",
    "    log_path = \"/Users/Dario/Desktop/log.txt\" # Where you want to save the log file\n",
    "\n",
    "    log = open(log_path, \"w\")# A log file:  In case the some error occur we can see at which point it ccured\n",
    "    \n",
    "    i = 10000 # Change with 0 if you are working with all 30000 files or with the starting number of your file\n",
    "    for file in sorted(os.listdir(path)):\n",
    "\n",
    "        if file.startswith('article_'): # there is a hidden file whith another name, in this way we avoid to open it\n",
    "\n",
    "            site = BeautifulSoup(open(\"\".join([path, str(file)])), \"html.parser\") # Let's open each page\n",
    "            \n",
    "            log.write(\"\".join([path, str(file)]) + \"\\n\")\n",
    "\n",
    "            try:\n",
    "                with open(\"\".join([save_in, \"article_\", str(i), \".tsv\"]), \"w\") as file:\n",
    "                    tsv_output = csv.writer(file, delimiter = '\\t')\n",
    "                    clean_el = clean_all(site) # Let's call the function that clean our data from \"impurities\"\n",
    "                    intro_plot = [clean_el[0], clean_el[1]] \n",
    "                    info_box = [info for _, info in clean_el[2].items()]\n",
    "                    elements = [info_box[0]] + intro_plot + info_box[1:] # Add as first element the title\n",
    "                    tsv_output.writerow(elements)\n",
    "\n",
    "            except Exception: # Let's see what is went wrong and break the cycle\n",
    "                log.close()\n",
    "                with open(log_path, \"r\") as log:\n",
    "                    lines = log.read().splitlines() # Read only the last line of the log (it contains the article with the error)\n",
    "                    last_line = lines[-1]\n",
    "                    print(\"The error is in: \", last_line, traceback.print_exc())\n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "            \n",
    "    log.close()\n",
    "    print(\"Finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "save_as_tsv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
