{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6ernczscIiT"
   },
   "outputs": [],
   "source": [
    "#this is the index 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2pEF9lqpifnP"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFjax9yJifwi"
   },
   "outputs": [],
   "source": [
    "def cleaner(text):#input: a text to be clean:\n",
    "                    #output: a list of words of the text cleaned\n",
    "    text=text.lower()   \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    clean_text=\"\".join([ch if ch.isalnum() else \" \" for ch in text]) #trasform all non alnumeric character\n",
    "                                                                    #into a space\n",
    "    words = word_tokenize(clean_text) \n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('na')\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # stemming of words\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(w) for w in words]\n",
    "    \n",
    "    return(stemmed)\n",
    "\n",
    "def number_document_html(file):#input: the name of the document; easy to obtain.. it's the file in: file in sorted(os.listdir(path))\n",
    "    if type(file)!=str:\n",
    "        raise Exception('file must have type str; {} obtained.'.format(type(file)))\n",
    "    start=len('article_')\n",
    "    return(int(file[start:-5])) #output: the number of the file\n",
    "\n",
    "def numer_document_tsv(file):#input: the name of the document; easy to obtain.. it's the file in: file in sorted(os.listdir(path))\n",
    "    if type(file)!=str:\n",
    "        raise Exception('file must have type str; {} obtained.'.format(type(file)))\n",
    "    start=len('article_')\n",
    "    return(int(file[start:-4])) #output: the number of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14Iedp0ri0Qx"
   },
   "outputs": [],
   "source": [
    "vocab={}\n",
    "index1={}\n",
    "index2={}\n",
    "al=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZVwLhg1ifzl"
   },
   "outputs": [],
   "source": [
    "path_vocab='/home/tiago/Scrivania/Libri Magistrale/1st semester/ADM/HomeWork3/vocabulary.pkl'\n",
    "path_index='/home/tiago/Scrivania/Libri Magistrale/1st semester/ADM/HomeWork3/inverted_index.pkl'\n",
    "path_tsv='/home/tiago/Scrivania/Libri Magistrale/1st semester/ADM/HomeWork3/fileTsv/'\n",
    "\n",
    "with open(path_index, 'rb') as f:\n",
    "    index1=pickle.load(f)\n",
    "with open(path_vocab, 'rb') as f:\n",
    "    vocab=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dlt5ZRG9if2U"
   },
   "outputs": [],
   "source": [
    "def cleaner_tsv_files_update_index_2(path_tsv): #input: the path where the file tsv are\n",
    "    for file in sorted(os.listdir(path_tsv))[:50]:\n",
    "        #print('sto pulendo il file: {}'.format(file))\n",
    "        if file.startswith(\"article_\"):\n",
    "            with open(path_tsv+file, \"r\", encoding = \"utf-8\") as f:\n",
    "                text = f.read().lower() # read the file                                           \n",
    "                clean_text_words=cleaner(text) #clean the text\n",
    "                updateIndex_2(clean_text_words, file) #--> updateIndex\n",
    "                \n",
    "def get_tfidf(listOfWords, num_word, num_doc):\n",
    "    freq_in_doc=index1[num_word][num_doc]\n",
    "    num_of_words_in_doc=len(listOfWords)\n",
    "    num_docs=30000\n",
    "    num_docs_with_w=len(index1[num_word])\n",
    "\n",
    "    tf=freq_in_doc/num_of_words_in_doc\n",
    "\n",
    "    idf=math.log(num_docs/num_docs_with_w)\n",
    "\n",
    "    return float(round(tf*idf, 9))\n",
    "                    \n",
    "def updateIndex_2(listOfWords, fileName):\n",
    "\n",
    "    num_doc=numer_document_tsv(fileName) #retrive the number of the file\n",
    "    for word in listOfWords:\n",
    "        num_word=vocab[word] #retrive the number of the word\n",
    "        \n",
    "        tfIdf=get_tfidf(listOfWords, num_word, num_doc)\n",
    "        \n",
    "\n",
    "        posts=index2.get(num_word)#try to obtain the value of the word key:\n",
    "        if posts==None: #if it is absent\n",
    "            index2[num_word]={num_doc:tfIdf} #add the key with value a dictionary{doc: frequenzy}\n",
    "        else:\n",
    "            post=index2[num_word].get(num_doc, None)# otherwise try to obtain the value of the document key\n",
    "            if post==None: #if it's absent:\n",
    "                index2[num_word][num_doc]=tfIdf #add the document key with frequency 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7Ds2oMejgn2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverted index creato correttamete!\n",
      "invert index salvato correttamete!\n"
     ]
    }
   ],
   "source": [
    "path_tsv='/home/tiago/Scrivania/Libri Magistrale/1st semester/ADM/HomeWork3/fileTsv/'\n",
    "path_index2='/home/tiago/Scrivania/Libri Magistrale/1st semester/ADM/HomeWork3/inverted_index2.pkl'\n",
    "#set your path\n",
    "\n",
    "\n",
    "cleaner_tsv_files_update_index_2(path_tsv) #cleane tsv file\n",
    "print('inverted index creato correttamete!')\n",
    "\n",
    "\n",
    "with open(path_index2, 'wb') as f: # and save index into a file\n",
    "        pickle.dump(index2, f, pickle.HIGHEST_PROTOCOL)\n",
    "print('invert index salvato correttamete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "3118\n",
      "131875\n"
     ]
    }
   ],
   "source": [
    "print(round(0.00123, 3))\n",
    "print(vocab['80'])\n",
    "print(len(index1))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "index2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
